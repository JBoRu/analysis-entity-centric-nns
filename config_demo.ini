# if main.py is run with option -r, hyperparameter values are randomly sampled.
# * '50-500' means sampling from uniform distribution over interval
# * '0.001~0.1' means sampling from log distribution over interval
# * 'yes|no', 'dot|cos' etc. means sampling from list of discrete options
# Please respect float = 0.0, int = 0 or it crashes :)

[Data]

dataset:       trial    # can currently be train or trial or test, referring to the semeval datasets
level:         scene   # can be scene or episode
folds:         2

[Training]

no shuffle:     no
epochs:         50
test every:     1
stop criterion: 5

batch size:     25
chunk size:     750
learning rate:  0.001   0.001~0.05
weight decay:   0.0     0.000001~0.01

optimizer:      adam
class weights:  yes      yes|no         # 'yes' to divide loss by sqrt of frequence of class (entity)
subsampling:    yes     yes|no          # 'yes' to substample first person pronouns

[Model]

#token emb:      200    100-300      # or comment this and uncomment the next line instead
token emb:      google_news       # requires file data/GoogleNews-vectors-negative300.bin.gz
speaker emb:    150

bidirectional:      yes
layers lstm:        1                   # multi-layer currently not implemented
hidden lstm 1:      500

dropout prob 1:  0.0     0.0-0.5
dropout prob 2:  0.0     0.0-0.2
nonlinearity 1:  relu    tanh|relu|sigmoid|no   # Nonlinearity before LSTM
nonlinearity 2:  relu    tanh|relu|sigmoid|no   # Nonlinearity after LSTM

attention lstm:     no	   # dot|no|feedforward
attention window:	yes     yes|no
window size:	    20	    10-80
nonlinearity a:     relu    tanh|relu|no   # Nonlinearity used in feedforward attention type

entity library:         static    dynamic|static|no     # can be no, static, dynamic

gate type:              dot         dot|cos|mlp       # can be dot, cos, mlp
gate softmax:           yes         yes|no     # for additional 'global' linear transformation + softmax over gate values (prior to updating library)
gate nonlinearity:      sigmoid     sigmoid|relu|tanh|no   # can be sigmoid, relu, tanh, or no
gate sum keys values:   yes         yes|no

# Only in case entity library is dynamic:
entlib weights:         yes     yes|no     # Whether library has weights (used as initialization of values, and optionally as keys)
entlib key:             yes	    yes|no        # Whether to use the weights as keys or merely as (initialization of) values (only if dynamic)
entlib normalization:   yes     yes|no      # (only if dynamic)
entlib value weights:	no	    yes|no		# Whether the library values are initialized with zeros or with the key weights

# Only if entity library has weights (e.g., always if static)
entlib shared:          no      yes|no     # Whether to share weights between entity library and speaker embedding.
entlib sharedinit:      yes     yes|no

# In case gate type is 'mlp':
gate mlp hidden:        200      100-300          # hidden layer size; to reduce hyperparams maybe this can be made dependent on entlib dim / speaker embedding size?
